#!/usr/bin/env python3
import os
import re
import json
import PyPDF2
from sentence_transformers import SentenceTransformer
import numpy as np
from filemaker_extractor import FileMakerExtractor
import logging
import sys

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PDFProcessor:
    def __init__(self):
        self.extractor = FileMakerExtractor()
        # Mod√®le d'embeddings l√©ger et efficace
        self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

    def clean_text(self, text):
        """Nettoie le texte extrait"""
        # Supprime les caract√®res ind√©sirables
        text = re.sub(r'\s+', ' ', text)  # Espaces multiples
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\‚Ç¨\%\(\)\-]', '', text)  # Caract√®res sp√©ciaux
        return text.strip()

    def extract_text_from_pdf(self, pdf_path):
        """Extrait le texte d'un PDF"""
        try:
            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)

                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    text += f"\n--- Page {page_num + 1} ---\n"
                    text += page_text

            return self.clean_text(text)

        except Exception as e:
            logger.error(f"‚ùå Erreur extraction PDF {pdf_path}: {str(e)}")
            return ""

    def chunk_text(self, text, chunk_size=800, overlap=100):
        """D√©coupe intelligemment le texte en chunks"""
        # Priorit√© aux sections financi√®res
        sections = re.split(r'\n(?=.*(?:√âVOLUTION|R√âSULTATS|BILAN|CAPITAUX|ACTIF|PASSIF).*)\n', text)

        chunks = []
        for section in sections:
            if len(section) <= chunk_size:
                chunks.append(section)
            else:
                # D√©coupe par phrases si trop long
                sentences = re.split(r'(?<=[.!?])\s+', section)
                current_chunk = ""

                for sentence in sentences:
                    if len(current_chunk + sentence) <= chunk_size:
                        current_chunk += sentence + " "
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence + " "

                if current_chunk:
                    chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 50]

    def generate_embeddings(self, texts):
        """G√©n√®re les embeddings pour une liste de textes"""
        embeddings = self.embedding_model.encode(texts)
        return embeddings

    def process_document(self, document_record, doc_index, total_docs):
        """Traite un document complet"""
        record_id = document_record['recordId']
        field_data = document_record['fieldData']
        filename = field_data.get('Nom_fichier', 'Inconnu')
        pdf_url = field_data.get('fichier', '')
        existing_text = field_data.get('text', '')

        # ‚úÖ V√âRIFICATION ANTI-DOUBLON
        existing_chunks = self.extractor.get_chunks_for_document(record_id)
        if existing_chunks and len(existing_chunks) > 0:
            logger.info(f"‚è≠Ô∏è [{doc_index}/{total_docs}] {filename} - D√©j√† trait√© ({len(existing_chunks)} chunks)")
            return True

        logger.info(f"üîÑ [{doc_index}/{total_docs}] Nouveau traitement: {filename}")

        # Si pas de texte extrait, on utilise le PDF
        if not existing_text or len(existing_text.strip()) < 100:
            if pdf_url:
                # T√©l√©charger et extraire
                pdf_path = f"/tmp/{filename}"
                if self.extractor.download_pdf(pdf_url, pdf_path):
                    text = self.extract_text_from_pdf(pdf_path)
                    os.remove(pdf_path)  # Nettoie
                else:
                    logger.error(f"‚ùå Impossible de t√©l√©charger {filename}")
                    return False
            else:
                logger.error(f"‚ùå Pas d'URL PDF pour {filename}")
                return False
        else:
            text = existing_text

        if not text or len(text.strip()) < 100:
            logger.warning(f"‚ö†Ô∏è Texte insuffisant pour {filename}")
            return False

        # Chunking
        chunks = self.chunk_text(text)
        logger.info(f"üìù {len(chunks)} chunks cr√©√©s")

        if not chunks:
            logger.warning(f"‚ö†Ô∏è Aucun chunk cr√©√© pour {filename}")
            return False

        # G√©n√©ration des embeddings
        try:
            embeddings = self.generate_embeddings(chunks)
            logger.info(f"üßÆ Embeddings g√©n√©r√©s")
        except Exception as e:
            logger.error(f"‚ùå Erreur embeddings: {str(e)}")
            return False

        # Sauvegarde dans FileMaker
        success_count = 0
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            try:
                embedding_json = json.dumps(embedding.tolist())
                if self.extractor.create_chunk(record_id, chunk, i + 1, embedding_json):
                    success_count += 1
            except Exception as e:
                logger.error(f"‚ùå Erreur sauvegarde chunk {i + 1}: {str(e)}")

        logger.info(f"‚úÖ {success_count}/{len(chunks)} chunks sauvegard√©s")
        return success_count > 0


def main(start_index=0, batch_size=450):
    """Traitement principal avec pagination"""
    processor = PDFProcessor()

    if not processor.extractor.login():
        logger.error("‚ùå Connexion FileMaker √©chou√©e")
        return

    # R√©cup√®re TOUS les documents
    documents = processor.extractor.get_documents()
    total_docs = len(documents)

    if total_docs == 0:
        logger.error("‚ùå Aucun document trouv√©")
        processor.extractor.logout()
        return

    logger.info(f"üìÅ {total_docs} documents trouv√©s au total")

    # Calcul de la plage de traitement
    end_index = min(start_index + batch_size, total_docs)
    batch_docs = documents[start_index:end_index]

    logger.info(f"üéØ Traitement: documents {start_index + 1} √† {end_index}")
    logger.info(f"üìä Batch: {len(batch_docs)} documents √† traiter")

    # Traitement
    processed = 0
    skipped = 0
    errors = 0

    for i, doc in enumerate(batch_docs):
        doc_index = start_index + i + 1

        try:
            success = processor.process_document(doc, doc_index, total_docs)
            if success:
                processed += 1
            else:
                errors += 1
        except Exception as e:
            logger.error(f"üí• Erreur document {doc_index}: {str(e)}")
            errors += 1

    # R√©sum√© final
    logger.info(f"üèÅ R√âSUM√â du batch {start_index}-{end_index}:")
    logger.info(f"   ‚úÖ Trait√©s: {processed}")
    logger.info(f"   ‚è≠Ô∏è D√©j√† faits: {skipped}")
    logger.info(f"   ‚ùå Erreurs: {errors}")

    processor.extractor.logout()


if __name__ == "__main__":
    start = int(sys.argv[1]) if len(sys.argv) > 1 else 0
    batch = int(sys.argv[2]) if len(sys.argv) > 2 else 450
    main(start, batch)
