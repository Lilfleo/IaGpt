#!/usr/bin/env python3
import os
import re
import json
import PyPDF2
from sentence_transformers import SentenceTransformer
import numpy as np
from filemaker_extractor import FileMakerExtractor
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PDFProcessor:
    def __init__(self):
        self.extractor = FileMakerExtractor()
        # Mod√®le d'embeddings l√©ger et efficace
        self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

    def clean_text(self, text):
        """Nettoie le texte extrait"""
        # Supprime les caract√®res ind√©sirables
        text = re.sub(r'\s+', ' ', text)  # Espaces multiples
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\‚Ç¨\%\(\)\-]', '', text)  # Caract√®res sp√©ciaux
        return text.strip()

    def extract_text_from_pdf(self, pdf_path):
        """Extrait le texte d'un PDF"""
        try:
            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)

                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    text += f"\n--- Page {page_num + 1} ---\n"
                    text += page_text

            return self.clean_text(text)

        except Exception as e:
            logger.error(f"‚ùå Erreur extraction PDF {pdf_path}: {str(e)}")
            return ""

    def chunk_text(self, text, chunk_size=800, overlap=100):
        """D√©coupe intelligemment le texte en chunks"""
        # Priorit√© aux sections financi√®res
        sections = re.split(r'\n(?=.*(?:√âVOLUTION|R√âSULTATS|BILAN|CAPITAUX|ACTIF|PASSIF).*)\n', text)

        chunks = []
        for section in sections:
            if len(section) <= chunk_size:
                chunks.append(section)
            else:
                # D√©coupe par phrases si trop long
                sentences = re.split(r'(?<=[.!?])\s+', section)
                current_chunk = ""

                for sentence in sentences:
                    if len(current_chunk + sentence) <= chunk_size:
                        current_chunk += sentence + " "
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = sentence + " "

                if current_chunk:
                    chunks.append(current_chunk.strip())

        return [chunk for chunk in chunks if len(chunk.strip()) > 50]

    def generate_embeddings(self, texts):
        """G√©n√®re les embeddings pour une liste de textes"""
        embeddings = self.embedding_model.encode(texts)
        return embeddings

    def process_document(self, document_record):
        """Traite un document complet"""
        record_id = document_record['recordId']
        field_data = document_record['fieldData']
        filename = field_data.get('Nom_fichier', '')
        pdf_url = field_data.get('fichier', '')
        existing_text = field_data.get('text', '')

        logger.info(f"üîÑ Traitement: {filename}")

        # Si pas de texte extrait, on utilise le PDF
        if not existing_text or len(existing_text.strip()) < 100:
            if pdf_url:
                # T√©l√©charger et extraire
                pdf_path = f"/tmp/{filename}"
                if self.extractor.download_pdf(pdf_url, pdf_path):
                    text = self.extract_text_from_pdf(pdf_path)
                    os.remove(pdf_path)  # Nettoie
                else:
                    logger.error(f"‚ùå Impossible de t√©l√©charger {filename}")
                    return False
            else:
                logger.error(f"‚ùå Pas d'URL PDF pour {filename}")
                return False
        else:
            text = existing_text

        if not text or len(text.strip()) < 100:
            logger.warning(f"‚ö†Ô∏è Texte insuffisant pour {filename}")
            return False

        # Chunking
        chunks = self.chunk_text(text)
        logger.info(f"üìù {len(chunks)} chunks cr√©√©s pour {filename}")

        # G√©n√©ration des embeddings
        embeddings = self.generate_embeddings(chunks)

        # Sauvegarde dans FileMaker
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            embedding_json = json.dumps(embedding.tolist())
            self.extractor.create_chunk(record_id, chunk, i + 1, embedding_json)

        return True


def main():
    """Traitement principal"""
    processor = PDFProcessor()

    # Connexion FileMaker
    if not processor.extractor.login():
        logger.error("‚ùå Connexion FileMaker √©chou√©e")
        return

    # R√©cup√©ration des documents
    documents = processor.extractor.get_documents()
    logger.info(f"üìÅ {len(documents)} documents √† traiter")

    # Traitement (limitons √† 5 pour le test)
    for i, doc in enumerate(documents[:5]):
        logger.info(f"üîÑ Document {i + 1}/{min(5, len(documents))}")
        processor.process_document(doc)

    processor.extractor.logout()
    logger.info("‚úÖ Traitement termin√©")


if __name__ == "__main__":
    main()
