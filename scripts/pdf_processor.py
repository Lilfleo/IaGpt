#!/usr/bin/env python3
import os
import re
import json
import fitz  # PyMuPDF
import pdfplumber
from sentence_transformers import SentenceTransformer
import numpy as np
from filemaker_extractor import FileMakerExtractor
import logging
import sys

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PDFProcessor:
    def __init__(self):
        self.extractor = FileMakerExtractor()
        # Mod√®le d'embeddings sp√©cialis√© fran√ßais
        try:
            self.embedding_model = SentenceTransformer('dangvantuan/sentence-camembert-large')
        except:
            # Fallback vers le mod√®le original si le sp√©cialis√© n'est pas disponible
            self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            logger.info("üìù Utilisation du mod√®le d'embedding par d√©faut")

    def clean_text(self, text):
        """Nettoyage intelligent pr√©servant les informations financi√®res"""
        # Normalisation des nombres et devises
        text = re.sub(r'(\d+)\s*[,\.]\s*(\d+)', r'\1,\2', text)  # Normalise les nombres
        text = re.sub(r'(\d+)\s*‚Ç¨', r'\1‚Ç¨', text)  # Colle ‚Ç¨ aux nombres
        text = re.sub(r'(\d+)\s*%', r'\1%', text)  # Colle % aux nombres
        text = re.sub(r'(\d+)\s*M‚Ç¨', r'\1M‚Ç¨', text)  # Millions d'euros

        # R√©paration des mots coup√©s par les sauts de ligne
        text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
        text = re.sub(r'([^.!?])\n([a-z√†-√ø])', r'\1 \2', text)

        # Nettoyage des espaces
        text = re.sub(r' +', ' ', text)
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r'\t+', ' ', text)

        # Suppression des caract√®res ind√©sirables mais pr√©servation de la ponctuation financi√®re
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\‚Ç¨\%\(\)\-\n¬∞/]', '', text)

        return text.strip()

    def extract_text_from_pdf(self, pdf_path):
        """Extraction am√©lior√©e avec PyMuPDF et fallback pdfplumber"""
        try:
            text_content = []

            # Tentative avec PyMuPDF (meilleur pour la structure)
            try:
                doc = fitz.open(pdf_path)
                for page_num, page in enumerate(doc):
                    # Extraction avec pr√©servation de la mise en page
                    text_dict = page.get_text("dict")
                    page_text = self.reconstruct_text_with_structure(text_dict)
                    if page_text.strip():
                        text_content.append(f"\n--- Page {page_num + 1} ---\n{page_text}")
                doc.close()

                final_text = "\n".join(text_content)
                if len(final_text.strip()) > 100:
                    return self.clean_text(final_text)

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è PyMuPDF failed, trying pdfplumber: {str(e)}")

            # Fallback avec pdfplumber (meilleur pour les tableaux)
            try:
                with pdfplumber.open(pdf_path) as pdf:
                    pages_text = []
                    for page_num, page in enumerate(pdf.pages):
                        page_text = page.extract_text(layout=True)
                        if page_text and page_text.strip():
                            pages_text.append(f"\n--- Page {page_num + 1} ---\n{page_text}")

                    final_text = "\n".join(pages_text)
                    if len(final_text.strip()) > 100:
                        return self.clean_text(final_text)

            except Exception as e:
                logger.error(f"‚ùå pdfplumber aussi √©chou√©: {str(e)}")

            logger.error(f"‚ùå Toutes les m√©thodes d'extraction ont √©chou√© pour {pdf_path}")
            return ""

        except Exception as e:
            logger.error(f"‚ùå Erreur extraction PDF {pdf_path}: {str(e)}")
            return ""

    def reconstruct_text_with_structure(self, text_dict):
        """Reconstruit le texte en pr√©servant la structure des colonnes"""
        lines = []

        try:
            for block in text_dict.get("blocks", []):
                if "lines" in block:
                    for line in block["lines"]:
                        line_text = ""
                        for span in line.get("spans", []):
                            line_text += span.get("text", "") + " "
                        if line_text.strip():
                            lines.append(line_text.strip())

            return "\n".join(lines)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur reconstruction structure: {str(e)}")
            return ""

    def classify_content_type(self, text):
        """Classifie le type de contenu du chunk"""
        text_lower = text.lower()

        # Mots-cl√©s par cat√©gorie
        categories = {
            'pricing': ['prix', 'souscription', 'commission', 'frais', 'tarif', 'modalit√©s'],
            'performance': ['rendement', 'distribution', 'tri', 'rgi', 'performance', 'dividende'],
            'assets': ['acquisition', 'patrimoine', 'actif', 'immobilier', 'surface', 'locataire'],
            'financial_data': ['capitalisation', 'collecte', 'parts', 'euros', 'bilan', 'r√©sultat'],
            'editorial': ['√©ditorial', 'message', 'pr√©sident', 'directeur'],
            'legal': ['conditions', 'cession', 'retrait', 'fiscalit√©', 'r√®glement']
        }

        scores = {}
        for category, keywords in categories.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                scores[category] = score / len(keywords)

        if not scores:
            return 'general'

        return max(scores, key=scores.get)

    def extract_financial_entities(self, text):
        """Extrait les entit√©s financi√®res du texte"""
        return {
            'prices': re.findall(r'(\d+(?:[,\.]\d+)*)\s*‚Ç¨', text),
            'percentages': re.findall(r'(\d+(?:[,\.]\d+)*)\s*%', text),
            'dates': re.findall(r'(\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{4})', text),
            'years': re.findall(r'\b(20\d{2})\b', text),
            'quarters': re.findall(r'(\d+[er]*\s*trimestre\s*\d{4})', text, re.IGNORECASE),
            'amounts_millions': re.findall(r'(\d+(?:[,\.]\d+)*)\s*[Mm]‚Ç¨', text),
            'surfaces': re.findall(r'(\d+(?:[,\.]\d+)*)\s*m[¬≤2]', text)
        }

    def calculate_financial_importance(self, text):
        """Calcule un score d'importance financi√®re"""
        score = 0

        # Pr√©sence d'entit√©s financi√®res
        entities = self.extract_financial_entities(text)
        score += len(entities['prices']) * 2
        score += len(entities['percentages']) * 2
        score += len(entities['amounts_millions']) * 3

        # Mots-cl√©s importants
        important_keywords = [
            'prix de souscription', 'rendement', 'distribution', 'capitalisation',
            'tri', 'rgi', 'performance', 'acquisition', 'collecte'
        ]

        text_lower = text.lower()
        for keyword in important_keywords:
            if keyword in text_lower:
                score += 3

        return min(10, score)  # Score max de 10

    def chunk_text_intelligent(self, text, chunk_size=800, overlap=100):
        """Chunking intelligent par sections financi√®res"""

        # Patterns pour identifier les sections importantes
        section_patterns = {
            'chiffres_cles': r'(?i)(chiffres?\s+cl[√©e]s?|situation\s+au|r[√©e]sum[√©e]|les\s+chiffres)',
            'prix_tarifs': r'(?i)(prix\s+de\s+souscription|modalit[√©e]s\s+de\s+souscription|tarifs?)',
            'performance': r'(?i)(performance|rendement|distribution|r[√©e]sultats?|tri|rgi)',
            'patrimoine': r'(?i)(patrimoine|acquisitions?|actifs?|portefeuille|zoom\s+sur)',
            'editorial': r'(?i)([√©e]ditorial|message|pr[√©e]sident|directeur\s+g[√©e]n[√©e]ral)',
            'evolution': r'(?i)([√©e]volution|coup\s+d.oeil|nouvelles?\s+acquisitions?)',
            'conditions': r'(?i)(conditions?\s+de\s+cession|modalit[√©e]s|fiscalit[√©e])',
            'actualite': r'(?i)(actualit[√©e]|news|informations?\s+g[√©e]n[√©e]rales?)'
        }

        chunks = []
        processed_ranges = []

        # Premier passage : sections identifi√©es
        for section_name, pattern in section_patterns.items():
            matches = list(re.finditer(pattern, text))

            for match in matches:
                # Contexte √©tendu autour de la section
                start = max(0, match.start() - 150)
                end = min(len(text), match.start() + chunk_size + 200)

                # √âviter les chevauchements avec des sections d√©j√† trait√©es
                if any(start < proc_end and end > proc_start for proc_start, proc_end in processed_ranges):
                    continue

                # Chercher une fin de section ou de phrase naturelle
                chunk_end = self.find_natural_boundary(text, end)
                chunk_text = text[start:chunk_end].strip()

                if len(chunk_text) > 100:  # Chunk significatif
                    # Enrichissement avec m√©tadonn√©es
                    chunk_info = {
                        'text': chunk_text,
                        'section': section_name,
                        'content_type': self.classify_content_type(chunk_text),
                        'financial_entities': self.extract_financial_entities(chunk_text),
                        'financial_score': self.calculate_financial_importance(chunk_text),
                        'word_count': len(chunk_text.split())
                    }

                    chunks.append(chunk_info)
                    processed_ranges.append((start, chunk_end))

        # Deuxi√®me passage : chunking traditionnel pour le reste
        remaining_text = text
        for proc_start, proc_end in sorted(processed_ranges, reverse=True):
            remaining_text = remaining_text[:proc_start] + remaining_text[proc_end:]

        if remaining_text.strip():
            traditional_chunks = self.traditional_chunking(remaining_text, chunk_size, overlap)
            for chunk_text in traditional_chunks:
                chunk_info = {
                    'text': chunk_text,
                    'section': 'general',
                    'content_type': self.classify_content_type(chunk_text),
                    'financial_entities': self.extract_financial_entities(chunk_text),
                    'financial_score': self.calculate_financial_importance(chunk_text),
                    'word_count': len(chunk_text.split())
                }
                chunks.append(chunk_info)

        # Tri par importance financi√®re et suppression des doublons
        chunks = self.deduplicate_and_sort_chunks(chunks)

        # Retour du texte simple pour compatibilit√© avec le syst√®me existant
        return [chunk['text'] for chunk in chunks if len(chunk['text'].strip()) > 50]

    def find_natural_boundary(self, text, position):
        """Trouve une fronti√®re naturelle pour d√©couper le texte"""
        if position >= len(text):
            return len(text)

        # Chercher la fin du paragraphe le plus proche
        for i in range(position, min(position + 200, len(text))):
            if text[i:i + 2] == '\n\n':
                return i

        # Sinon, chercher la fin de phrase
        for i in range(position, min(position + 100, len(text))):
            if text[i] in '.!?' and i + 1 < len(text) and text[i + 1] in ' \n':
                return i + 1

        return position

    def traditional_chunking(self, text, chunk_size, overlap):
        """Chunking traditionnel am√©lior√©"""
        if len(text) <= chunk_size:
            return [text] if text.strip() else []

        chunks = []
        start = 0

        while start < len(text):
            end = min(start + chunk_size, len(text))

            # Ajuster pour ne pas couper au milieu d'une phrase
            if end < len(text):
                # Chercher la fin de phrase la plus proche
                for i in range(end, max(start + chunk_size - 200, start), -1):
                    if text[i] in '.!?' and i + 1 < len(text) and text[i + 1] in ' \n':
                        end = i + 1
                        break

            chunk = text[start:end].strip()
            if chunk and len(chunk) > 50:
                chunks.append(chunk)

            start = max(start + chunk_size - overlap, end - overlap)
            if start >= len(text):
                break

        return chunks

    def deduplicate_and_sort_chunks(self, chunks):
        """D√©duplique et trie les chunks par importance"""
        # D√©duplication bas√©e sur la similarit√© du contenu
        unique_chunks = []

        for chunk in chunks:
            is_duplicate = False
            chunk_words = set(chunk['text'].lower().split())

            for existing in unique_chunks:
                existing_words = set(existing['text'].lower().split())
                similarity = len(chunk_words & existing_words) / len(chunk_words | existing_words)

                if similarity > 0.8:  # 80% de similarit√© = doublon
                    is_duplicate = True
                    # Garder le chunk avec le meilleur score financier
                    if chunk['financial_score'] > existing['financial_score']:
                        unique_chunks.remove(existing)
                        unique_chunks.append(chunk)
                    break

            if not is_duplicate:
                unique_chunks.append(chunk)

        # Tri par score financier d√©croissant
        return sorted(unique_chunks, key=lambda x: x['financial_score'], reverse=True)

    def generate_embeddings(self, texts):
        """G√©n√®re les embeddings pour une liste de textes"""
        try:
            embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
            return embeddings
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration embeddings: {str(e)}")
            raise

    def process_document(self, document_record, doc_index, total_docs):
        """Traite un document complet"""
        record_id = document_record['recordId']
        field_data = document_record['fieldData']
        filename = field_data.get('Nom_fichier', 'Inconnu')
        pdf_url = field_data.get('fichier', '')
        existing_text = field_data.get('text', '')

        # ‚úÖ V√âRIFICATION ANTI-DOUBLON
        existing_chunks = self.extractor.get_chunks_for_document(record_id)
        if existing_chunks and len(existing_chunks) > 0:
            logger.info(f"‚è≠Ô∏è [{doc_index}/{total_docs}] {filename} - D√©j√† trait√© ({len(existing_chunks)} chunks)")
            return True

        logger.info(f"üîÑ [{doc_index}/{total_docs}] Nouveau traitement: {filename}")

        # Extraction du texte
        text = ""
        if existing_text and len(existing_text.strip()) >= 100:
            text = existing_text
            logger.info(f"üìù Utilisation du texte existant ({len(text)} caract√®res)")
        elif pdf_url:
            # T√©l√©charger et extraire avec la nouvelle m√©thode
            pdf_path = f"/tmp/{filename}"
            if self.extractor.download_pdf(pdf_url, pdf_path):
                text = self.extract_text_from_pdf(pdf_path)
                os.remove(pdf_path)  # Nettoie
                logger.info(f"üìÑ Texte extrait du PDF ({len(text)} caract√®res)")
            else:
                logger.error(f"‚ùå Impossible de t√©l√©charger {filename}")
                return False
        else:
            logger.error(f"‚ùå Pas de texte ni d'URL PDF pour {filename}")
            return False

        if not text or len(text.strip()) < 100:
            logger.warning(f"‚ö†Ô∏è Texte insuffisant pour {filename}")
            return False

        # Chunking intelligent
        try:
            chunks = self.chunk_text_intelligent(text)
            logger.info(f"üìù {len(chunks)} chunks cr√©√©s avec chunking intelligent")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chunking intelligent √©chou√©, fallback traditionnel: {str(e)}")
            chunks = self.traditional_chunking(text, 800, 100)
            logger.info(f"üìù {len(chunks)} chunks cr√©√©s avec chunking traditionnel")

        if not chunks:
            logger.warning(f"‚ö†Ô∏è Aucun chunk cr√©√© pour {filename}")
            return False

        # G√©n√©ration des embeddings
        try:
            embeddings = self.generate_embeddings(chunks)
            logger.info(f"üßÆ Embeddings g√©n√©r√©s pour {len(chunks)} chunks")
        except Exception as e:
            logger.error(f"‚ùå Erreur embardings: {str(e)}")
            return False

        # Sauvegarde dans FileMaker
        success_count = 0
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            try:
                embedding_json = json.dumps(embedding.tolist())
                if self.extractor.create_chunk(record_id, chunk, i + 1, embedding_json):
                    success_count += 1
            except Exception as e:
                logger.error(f"‚ùå Erreur sauvegarde chunk {i + 1}: {str(e)}")

        success_rate = (success_count / len(chunks)) * 100
        logger.info(f"‚úÖ {success_count}/{len(chunks)} chunks sauvegard√©s ({success_rate:.1f}%)")

        return success_count > 0


def main(start_index=0, batch_size=450):
    """Traitement principal avec pagination"""
    processor = PDFProcessor()

    if not processor.extractor.login():
        logger.error("‚ùå Connexion FileMaker √©chou√©e")
        return

    # R√©cup√®re TOUS les documents
    documents = processor.extractor.get_documents()
    total_docs = len(documents)

    if total_docs == 0:
        logger.error("‚ùå Aucun document trouv√©")
        processor.extractor.logout()
        return

    logger.info(f"üìÅ {total_docs} documents trouv√©s au total")

    # Calcul de la plage de traitement
    end_index = min(start_index + batch_size, total_docs)
    batch_docs = documents[start_index:end_index]

    logger.info(f"üéØ Traitement: documents {start_index + 1} √† {end_index}")
    logger.info(f"üìä Batch: {len(batch_docs)} documents √† traiter")

    # Traitement
    processed = 0
    errors = 0

    for i, doc in enumerate(batch_docs):
        doc_index = start_index + i + 1

        try:
            success = processor.process_document(doc, doc_index, total_docs)
            if success:
                processed += 1
            else:
                errors += 1
        except Exception as e:
            logger.error(f"üí• Erreur document {doc_index}: {str(e)}")
            errors += 1

    # R√©sum√© final
    logger.info(f"üèÅ R√âSUM√â du batch {start_index}-{end_index}:")
    logger.info(f"   ‚úÖ Trait√©s avec succ√®s: {processed}")
    logger.info(f"   ‚ùå Erreurs: {errors}")
    logger.info(
        f"   üìä Taux de succ√®s: {(processed / (processed + errors) * 100):.1f}%" if (processed + errors) > 0 else "")

    processor.extractor.logout()


if __name__ == "__main__":
    start = int(sys.argv[1]) if len(sys.argv) > 1 else 0
    batch = int(sys.argv[2]) if len(sys.argv) > 2 else 450
    main(start, batch)
