#!/usr/bin/env python3
from flask import Flask, request, jsonify
import sys
import os
import locale
import json
import requests
import numpy as np

# Configuration locale
locale.setlocale(locale.LC_ALL, 'fr_FR.UTF-8')
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.filemaker_extractor import FileMakerExtractor
from sentence_transformers import SentenceTransformer

app = Flask(__name__)


class RAGSearcher:
    """Service de recherche RAG avec FileMaker et IA"""

    def __init__(self):
        print("üîß Initialisation du service RAG...")
        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        print("‚úÖ Mod√®le d'embedding charg√©")

    def connect_filemaker(self):
        """√âtablit une connexion fra√Æche √† FileMaker"""
        extractor = FileMakerExtractor()
        if extractor.connect():
            print("‚úÖ Connexion FileMaker √©tablie")
            return extractor
        else:
            print("‚ùå √âchec connexion FileMaker")
            return None

    def search(self, question):
        """Recherche principale avec gestion compl√®te"""
        extractor = None

        try:
            print(f"üîç D√©but de recherche pour: '{question}'")

            # 1Ô∏è‚É£ CONNEXION FILEMAKER
            extractor = self.connect_filemaker()
            if not extractor:
                return self.error_response(question, "Impossible de se connecter √† la base de donn√©es")

            # 2Ô∏è‚É£ RECHERCHE TEXTUELLE PR√âALABLE
            print(f"üîç Phase 1: Recherche textuelle...")
            raw_chunks = extractor.search_chunks_smart(question, limit=1000)

            if not raw_chunks:
                print("‚ùå Aucun chunk trouv√©")
                return self.empty_response(question, "Aucune information trouv√©e dans la base de donn√©es")

            print(f"üìä {len(raw_chunks)} chunks trouv√©s par recherche textuelle")

            # 3Ô∏è‚É£ CALCUL SIMILARIT√âS S√âMANTIQUES
            print(f"üßÆ Phase 2: Calcul des similarit√©s...")
            top_chunks = self.calculate_similarities(question, raw_chunks)
            # APR√àS la ligne top_chunks = self.calculate_similarities(...)
            print("üîç DEBUG - TOP 3 CHUNKS TROUV√âS :")
            for i, chunk in enumerate(top_chunks[:3]):
                chunk_data = chunk['fieldData'] if 'fieldData' in chunk else chunk
                print(f"  Chunk {i + 1}: Doc {chunk_data.get('idDocument', 'N/A')}")
                print(f"    Similarit√©: {chunk.get('similarity', 'N/A'):.4f}")
                print(f"    Texte (50 chars): '{chunk_data.get('Text', '')[:50]}...'")
                print(f"    A des embeddings: {'OUI' if chunk_data.get('EmbeddingJson') else 'NON'}")
                print()

            if not top_chunks:
                return self.empty_response(question, "Aucun chunk avec embedding valide trouv√©")

            # 4Ô∏è‚É£ DEBUG DES CHUNKS S√âLECTIONN√âS
            self.debug_chunks(top_chunks[:5], question)

            # 5Ô∏è‚É£ G√âN√âRATION DE LA R√âPONSE
            context = self.prepare_context(top_chunks[:5])
            response = self.generate_answer(question, context)

            # 6Ô∏è‚É£ R√âSULTAT FINAL
            return {
                "question": question,
                "response": response,
                "sources": [chunk['document_name'] for chunk in top_chunks[:5]],
                "chunks_analyzed": len(top_chunks),
                "status": "success"
            }

        except Exception as e:
            print(f"‚ùå ERREUR GLOBALE: {e}")
            return self.error_response(question, f"Erreur interne: {str(e)}")

        finally:
            # 7Ô∏è‚É£ NETTOYAGE
            if extractor:
                extractor.logout()
                print("üîå Connexion FileMaker ferm√©e")

    def calculate_similarities(self, question, raw_chunks, top_k=20):
        """Calcule les similarit√©s s√©mantiques avec debug d√©taill√©"""
        print(f"\nüîç DEBUG CALCULATE_SIMILARITIES")
        print(f"üìä Nombre de chunks re√ßus: {len(raw_chunks)}")

        # Embedding de la question
        question_embedding = self.model.encode([question])
        question_vec = question_embedding[0]
        print(f"üßÆ Question embedding shape: {len(question_vec)}")

        similarities = []
        processed = 0
        errors = 0

        for i, chunk_record in enumerate(raw_chunks[:5]):  # Debug sur les 5 premiers
            try:
                print(f"\n--- CHUNK {i + 1} DEBUG ---")

                # Extraction des donn√©es du chunk
                chunk_data = chunk_record['fieldData'] if 'fieldData' in chunk_record else chunk_record
                print(f"üìÑ Record ID: {chunk_record.get('recordId')}")
                print(f"üìÑ Chunk data keys: {list(chunk_data.keys())}")

                text = chunk_data.get('Text', '').strip()
                embedding_json = chunk_data.get('EmbeddingJson', '')
                doc_id = chunk_data.get('idDocument', 'N/A')

                print(f"üìù Text pr√©sent: {'OUI' if text else 'NON'} ({len(text)} chars)")
                print(f"üßÆ EmbeddingJson type: {type(embedding_json)}")
                print(f"üßÆ EmbeddingJson pr√©sent: {'OUI' if embedding_json else 'NON'}")

                if embedding_json:
                    print(f"üßÆ EmbeddingJson length: {len(str(embedding_json))} caract√®res")
                    print(f"üßÆ EmbeddingJson preview: {str(embedding_json)[:100]}...")

                    # Test de parsing
                    if isinstance(embedding_json, str):
                        print("üîß Tentative de parsing JSON string...")
                        chunk_embedding = np.array(json.loads(embedding_json))
                    else:
                        print("üîß D√©j√† un objet, conversion directe...")
                        chunk_embedding = np.array(embedding_json)

                    print(f"‚úÖ Embedding pars√© OK, shape: {chunk_embedding.shape}")

                    # Validation des dimensions
                    if len(chunk_embedding) != len(question_vec):
                        print(f"‚ùå ERREUR DIMENSION: chunk={len(chunk_embedding)} vs question={len(question_vec)}")
                        continue

                    # Calcul de similarit√© cosinus
                    similarity = np.dot(question_vec, chunk_embedding) / (
                            np.linalg.norm(question_vec) * np.linalg.norm(chunk_embedding)
                    )
                    print(f"üéØ Similarit√© calcul√©e: {similarity:.6f}")

                    similarities.append({
                        'similarity': float(similarity),
                        'text': text,
                        'document_id': doc_id,
                        'document_name': f"Doc_{doc_id}",
                        'raw_data': chunk_data
                    })
                    processed += 1

                else:
                    print(f"‚ùå Pas d'embedding - SKIPP√â")

            except json.JSONDecodeError as e:
                print(f"‚ùå Erreur JSON parsing: {e}")
                errors += 1
            except Exception as e:
                print(f"‚ùå Erreur g√©n√©rale: {e}")
                errors += 1

        print(f"\nüìä R√âSUM√â DEBUG:")
        print(f"   ‚úÖ Trait√©s avec succ√®s: {processed}")
        print(f"   ‚ùå Erreurs: {errors}")

        # Continuez le traitement pour TOUS les chunks (pas juste les 5 premiers)
        print(f"\nüîÑ TRAITEMENT COMPLET DE TOUS LES CHUNKS...")

        for chunk_record in raw_chunks:
            try:
                chunk_data = chunk_record['fieldData'] if 'fieldData' in chunk_record else chunk_record
                text = chunk_data.get('Text', '').strip()
                embedding_json = chunk_data.get('EmbeddingJson', '').strip()
                doc_id = chunk_data.get('idDocument', 'N/A')

                if not text or not embedding_json:
                    continue

                # Parsing de l'embedding
                chunk_embedding = np.array(json.loads(embedding_json)) if isinstance(embedding_json, str) else np.array(
                    embedding_json)

                # Calcul de similarit√©
                similarity = np.dot(question_vec, chunk_embedding) / (
                        np.linalg.norm(question_vec) * np.linalg.norm(chunk_embedding)
                )

                similarities.append({
                    'similarity': float(similarity),
                    'text': text,
                    'document_id': doc_id,
                    'document_name': f"Doc_{doc_id}",
                    'raw_data': chunk_data
                })

            except (json.JSONDecodeError, ValueError, KeyError):
                continue

        if not similarities:
            print("‚ùå AUCUNE SIMILARIT√â CALCUL√âE")
            return []

        # Tri par similarit√© d√©croissante
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        top_chunks = similarities[:top_k]

        print(f"üéØ Top {len(top_chunks)} chunks s√©lectionn√©s sur {len(similarities)} total")
        return top_chunks

    def debug_chunks(self, chunks, question):
        """Affiche le debug des chunks s√©lectionn√©s"""
        print(f"\nüîç DEBUG - TOP {len(chunks)} CHUNKS:")

        # Mots-cl√©s de recherche pour le debug
        question_words = question.lower().split()

        for i, chunk in enumerate(chunks, 1):
            text_preview = chunk['text'][:200]
            similarity = chunk['similarity']
            doc_name = chunk['document_name']

            print(f"--- CHUNK {i} ---")
            print(f"üìÑ Document: {doc_name}")
            print(f"üéØ Similarit√©: {similarity:.4f}")
            print(f"üìù Extrait: {text_preview}...")

            # Recherche de mots-cl√©s dans le texte
            text_lower = chunk['text'].lower()
            keywords_found = [word for word in question_words if word in text_lower and len(word) > 2]

            if keywords_found:
                print(f"üîç Mots-cl√©s trouv√©s: {keywords_found}")
            else:
                print(f"‚ö†Ô∏è  Aucun mot-cl√© direct trouv√©")

            print()

    def prepare_context(self, chunks):
        """Pr√©pare le contexte pour l'IA √† partir des chunks"""
        if not chunks:
            return "Aucun contexte disponible."

        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            text = chunk['text'][:600]  # Limite √† 600 chars par chunk
            doc_name = chunk['document_name']
            similarity = chunk['similarity']

            context_parts.append(
                f"[Source {i} - {doc_name} (pertinence: {similarity:.3f})]\n{text}"
            )

        return "\n\n" + "=" * 50 + "\n\n".join(context_parts)

    def generate_answer(self, question, context):
        """G√©n√®re la r√©ponse avec Ollama"""
        print("ü§ñ G√©n√©ration de la r√©ponse avec Ollama...")

        prompt = f"""Contexte: {context}

        Question: {question}

        R√©ponds en fran√ßais en te basant uniquement sur les informations du contexte. Si tu dois faire un calcul, montre-le simplement.

        R√©ponse:"""

        try:
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': 'mistral:7b-instruct',
                    'prompt': prompt,
                    'stream': False,
                    'options': {
                        'temperature': 0.1,  # Plus d√©terministe
                        'num_ctx': 4096  # Plus de contexte
                    }
                },
                timeout=180
            )

            if response.status_code == 200:
                result = response.json().get('response', '').strip()
                print("‚úÖ R√©ponse g√©n√©r√©e avec succ√®s")
                return result if result else "Erreur: R√©ponse vide g√©n√©r√©e"
            else:
                print(f"‚ùå Erreur Ollama: {response.status_code}")
                return "Erreur: Service IA indisponible"

        except requests.exceptions.Timeout:
            print("‚ùå Timeout Ollama")
            return "Erreur: Le service IA a pris trop de temps √† r√©pondre"
        except Exception as e:
            print(f"‚ùå Exception Ollama: {e}")
            return f"Erreur service IA: {str(e)}"

    def error_response(self, question, message):
        """G√©n√®re une r√©ponse d'erreur standardis√©e"""
        return {
            "question": question,
            "response": message,
            "sources": [],
            "chunks_analyzed": 0,
            "status": "error"
        }

    def empty_response(self, question, message):
        """G√©n√®re une r√©ponse vide standardis√©e"""
        return {
            "question": question,
            "response": message,
            "sources": [],
            "chunks_analyzed": 0,
            "status": "no_results"
        }


# Instance globale du service
print("üöÄ Cr√©ation de l'instance RAG...")
searcher = RAGSearcher()
print("‚úÖ Service RAG initialis√©")


@app.route('/search', methods=['POST'])
def search_endpoint():
    """Endpoint principal de recherche"""
    try:
        print("\n" + "=" * 60)
        print("üîç NOUVELLE REQU√äTE RE√áUE")
        print("=" * 60)

        # R√©cup√©ration de la question
        data = request.get_json()
        if not data:
            return jsonify({"error": "Pas de donn√©es JSON re√ßues"}), 400

        question = data.get('question', '').strip()

        if not question:
            return jsonify({"error": "Question manquante ou vide"}), 400

        print(f"üìù Question: '{question}'")

        # Lancement de la recherche
        result = searcher.search(question)

        print(f"‚úÖ Recherche termin√©e - Status: {result.get('status', 'unknown')}")
        print("=" * 60)

        return jsonify(result)

    except Exception as e:
        print(f"‚ùå ERREUR ENDPOINT: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": f"Erreur serveur: {str(e)}",
            "status": "error"
        }), 500


@app.route('/health', methods=['GET'])
def health():
    """Endpoint de sant√© du service"""
    try:
        # Test de connexion FileMaker
        extractor = FileMakerExtractor()
        fm_ok = extractor.connect()
        if fm_ok:
            extractor.logout()

        # Test Ollama
        try:
            ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)
            ollama_ok = ollama_response.status_code == 200
        except:
            ollama_ok = False

        return jsonify({
            "status": "OK" if (fm_ok and ollama_ok) else "PARTIAL",
            "service": "RAG API",
            "components": {
                "filemaker": "OK" if fm_ok else "ERROR",
                "ollama": "OK" if ollama_ok else "ERROR",
                "embeddings": "OK"
            },
            "version": "2.0"
        })

    except Exception as e:
        return jsonify({
            "status": "ERROR",
            "error": str(e)
        }), 500


if __name__ == '__main__':
    print("\nüöÄ D√âMARRAGE DU SERVEUR RAG")
    print("=" * 40)
    print("üì° URL: http://localhost:9000")
    print("üîç Recherche: POST /search")
    print("üíö Sant√©: GET /health")
    print("=" * 40)

    app.run(host='0.0.0.0', port=9000, debug=True)
